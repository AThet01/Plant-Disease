# -*- coding: utf-8 -*-
"""pdc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kyhokqr4IPtCguQpzMqDbv_-18oklsKy
"""

# Install necessary libraries
!pip install tensorflow streamlit numpy pillow scikit-learn pyngrok

!pip install -q addict

!pip install tensorflow streamlit pillow pyngrok

from google.colab import drive
drive.mount('/content/drive')

!pip install -q addict

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchvision import models
from PIL import Image
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2
import numpy as np
import os
import random
from addict import Dict
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm
import logging

def seed_everything(seed:int=42) -> None:
    random.seed(seed)
    os.environ['PYTHONASSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


def get_optimizer(model:torch.nn.Module, name:str="SGD", parameters:dict={}) -> torch.optim.Optimizer:
    optimizers = {
        "SGD": torch.optim.SGD,
        "AdamW": torch.optim.AdamW,
        "Adam": torch.optim.Adam,
        "RMSprop": torch.optim.RMSprop,
    }

    instance = optimizers.get(name, "SGD")
    optimizer = instance(model.parameters(), **parameters)

    return optimizer


def get_scheduler(optimizer:torch.optim.Optimizer, name:str, parameters:dict):
    schedulers = {
        "ReduceLROnPlateau": torch.optim.lr_scheduler.ReduceLROnPlateau,
        "LambdaLR": torch.optim.lr_scheduler.LambdaLR,
        "StepLR": torch.optim.lr_scheduler.StepLR,
        "ExponentialLR": torch.optim.lr_scheduler.ExponentialLR,
        "MultiplicativeLR": torch.optim.lr_scheduler.MultiplicativeLR,
        "MultiStepLR": torch.optim.lr_scheduler.MultiStepLR,
    }

    instance = schedulers[name]
    scheduler = instance(optimizer, **parameters)

    return scheduler



def accuracy_score(predictions:torch.Tensor, targets:torch.Tensor) -> torch.Tensor:
    amount = (predictions == targets).sum()
    accuracy = amount / targets.size(0)

    return accuracy



def hide_spines(ax, spines=["top", "right", "left", "bottom"]):
    for spine in spines:
        ax.spines[spine].set_visible(False)

def plot_images(rows, cols, indexes, class_=0):
    min_index = min(indexes)
    max_index = max(indexes)
    fig = plt.figure(figsize=(3*cols, 3*rows))
    for i in range(*indexes):
        item = train_dataset[i]
        image = item.image
        label = item.label

        if label == class_:
            ax = fig.add_subplot(rows, cols, (i - min_index)+1)
            ax.imshow(image.permute(1, 2, 0))
            ax.xaxis.set_visible(False)
            ax.yaxis.set_visible(False)

    fig.text(s=f"{train_dataset.labels[class_]} leaves", x=0.125, y=0.9, fontweight="bold", fontfamily="serif", fontsize=20)
    fig.show()


def get_logger(name:str=__name__, format:str="[%(asctime)s][%(levelname)s]: %(message)s") -> logging.Logger:
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter(format)

    file_handler = logging.FileHandler(name)
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)

    stream_handler = logging.StreamHandler()
    stream_handler.setLevel(logging.INFO)
    stream_handler.setFormatter(formatter)

    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)

    logger.propagate = False

    return logger

config = Dict({
    "train_path": "/content/drive/MyDrive/Plant Disease/Train",
    "test_path": "/content/drive/MyDrive/Plant Disease/Test (1)",
    "validation_path": "/content/drive/MyDrive/Plant Disease/Validation"
})

train_config = Dict({
    "device": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    "epochs": 5,
    "seed": 2021,
    "image_shape": (128, 128),
    "image_channels": 3,
    "num_workers": 0,
    "batch_size": 32,

    "augmentations": A.Compose([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        #A.Blur(p=1),
        ToTensorV2(),
    ]),
    "optimizer": {
        "type": "AdamW",
        "parameters": {
            "lr": 0.001,
            "weight_decay": 0.01,
        }
    },

    "scheduler": {
        "type": "ReduceLROnPlateau",
        "parameters": {
            "patience": 2,
            "mode": "min",
            "factor": 0.1,
        }
    }
})


seed_everything(train_config.seed)

class PlantDiseaseDataset(Dataset):
    def __init__(self, path, augmentations=None, image_shape=(256, 256), channels="RGB"):
        self.__images_labels = []
        self.image_shape = image_shape
        self.channels = channels
        self.augmentations = augmentations

        if os.path.exists(path):
            self.labels = os.listdir(path)
            for label in self.labels:
                label_path = os.path.join(path, label)
                if os.path.isdir(label_path):
                    files = os.listdir(label_path)
                    for file in files:
                        if file.endswith("jpg") or file.endswith("png"):
                            image_path = os.path.join(label_path, file)
                            self.__images_labels.append((image_path, label))
                        else:
                            pass
                else:
                    pass

        else:
            pass

    def _load(self, path, channels="RGB"):
        width, height = self.image_shape
        loader = A.Compose([
            A.Resize(width=width, height=height),
            ToTensorV2(),
        ])

        image_array = np.array(Image.open(path).convert(channels))
        return loader(image=image_array)["image"]

    def __len__(self):
        return len(self.__images_labels)

    def __getitem__(self, index):
        path, label = self.__images_labels[index]
        image = self._load(path)

        if self.augmentations is not None:
            image = image.permute(1, 2, 0).numpy()
            image = self.augmentations(image=image)["image"]

        label = self.labels.index(label)

        return Dict({
            "image": image,
            "label": label,
        })


def collate_fn(batch):
    all_images, all_labels = [], []
    for item in batch:
        image = item.image
        label = item.label

        all_images.append(item.image.tolist())
        all_labels.append(label)

    return {
        "images": torch.tensor(all_images),
        "labels": torch.tensor(all_labels, dtype=torch.int8)
    }

train_dataset = PlantDiseaseDataset(path=config.train_path,
                                    image_shape=train_config.image_shape,
                                    channels=train_config.image_channels)

label_pathes = [os.path.join(config.train_path, label) for label in train_dataset.labels]
label_files = [os.listdir(path) for path in label_pathes]
amount = [len(files) for files in label_files]

palette = sns.color_palette(["#5FB924", "#AB4800", "#B2BBAC"])
fig = plt.figure(figsize=(7, 7))
ax = fig.add_subplot()
ax.grid(color="lightgrey", axis="both", alpha=0.8, zorder=0)
sns.barplot(x=train_dataset.labels, y=amount, palette=palette,  ec="#000", linewidth=1.5, zorder=2, ax=ax)
ax.xaxis.set_tick_params(labelsize=14, size=0, pad=10)
ax.yaxis.set_tick_params(labelsize=12, size=0, pad=5)
ax.set_yticks(list(range(0, 450, 50)))
ax.set_title(f"Classes Distribution", fontsize=20, fontweight="bold", fontfamily="serif", loc="left", y=1.01)
ax.set_xlabel("Classes", fontsize=15, fontfamily="serif", labelpad=5)
ax.set_ylabel("Count", fontsize=15, fontfamily="serif", labelpad=5)
hide_spines(ax)
fig.show()

plot_images(rows=5, cols=5, indexes=(0, 25), class_=0)

plot_images(rows=5, cols=5, indexes=(500, 525), class_=1)

plot_images(rows=5, cols=5, indexes=(len(train_dataset)-25, len(train_dataset)), class_=2)

train_dataset = PlantDiseaseDataset(path=config.train_path,
                                    augmentations=train_config.augmentations,
                                    image_shape=train_config.image_shape,
                                    channels=train_config.image_channels)

validation_dataset = PlantDiseaseDataset(path=config.validation_path,
                                         augmentations=train_config.augmentations,
                                         image_shape=train_config.image_shape,
                                         channels=train_config.image_channels)

test_dataset = PlantDiseaseDataset(path=config.test_path,
                                   augmentations=train_config.augmentations,
                                   image_shape=train_config.image_shape,
                                   channels=train_config.image_channels)

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=train_config.batch_size,
                          num_workers=train_config.num_workers,
                          pin_memory=True,
                          shuffle=True,
                          collate_fn=collate_fn)

validation_loader = DataLoader(dataset=validation_dataset,
                               batch_size=train_config.batch_size*2,
                               num_workers=train_config.num_workers,
                               pin_memory=True,
                               shuffle=False,
                               collate_fn=collate_fn)

test_loader = DataLoader(dataset=test_dataset,
                         batch_size=train_config.batch_size*2,
                         num_workers=train_config.num_workers,
                         pin_memory=True,
                         shuffle=False,
                         collate_fn=collate_fn)

class PlantDiseaseModel(nn.Module):
    def __init__(self, classes=2):
        super(PlantDiseaseModel, self).__init__()
        self.model = models.resnet34(pretrained=True)

        for parameter in self.model.parameters():
            parameter.require_grad = False

        in_features = self.model.fc.in_features
        self.model.fc = nn.Sequential(
            nn.Linear(in_features=in_features, out_features=classes),
            nn.Softmax(dim=1)
        )

    def forward(self, image):
        output = self.model(image)
        return output

import torch
from tqdm import tqdm
from collections import defaultdict  # This will allow easy dictionary updates

class Trainer:
    def __init__(self, model, criterion, optimizer, metric, scheduler=None, logger=None, device="cpu"):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.logger = logger
        self.device = torch.device(device)
        self.best_validation_loss = float('inf')  # Initialize to a large value
        self.metric = metric
        self.history = defaultdict(list)  # Initialize to use defaultdict for dynamic keys

    def __log(self, logs):
        """ Logs training and evaluation metrics """
        for k, v in logs.items():
            self.history[k].append(v)

    def evaluate(self, loader):
        """ Evaluates the model on a given dataset """
        self.model.to(self.device)
        self.model.eval()
        loss, score, length = 0, 0, len(loader)

        # Debug: Print loader length to help debug ZeroDivisionError
        if length == 0:
            print("Warning: Validation loader is empty.")
            return 0, 0

        with torch.no_grad():
            loop = tqdm(loader, position=0, colour="BLACK", desc="Evaluating: ", leave=True)
            for batch in loop:
                if self.device.type != "cpu":
                    torch.cuda.empty_cache()  # Clear GPU memory

                images = batch["images"].float().to(self.device)
                labels = batch["labels"].long().to(self.device)

                probabilities = self.model(images).float().to(self.device)
                predictions = torch.argmax(probabilities, dim=1)

                batch_loss = self.criterion(probabilities, labels)
                loss += batch_loss.item()

                batch_score = self.metric(predictions, labels).item()
                score += batch_score

            # Check if length is zero before dividing
            if length == 0:
                print("Error: Division by zero. Length of loader is 0.")
                return 0, 0

            loss /= length
            score /= length

        return loss, score

    def fit(self, train_loader, validation_loader=None, epochs=10):
        """ Trains the model and evaluates at the end of each epoch """
        self.model.to(self.device)
        train_length = len(train_loader)

        # Debug: Check if train_loader is empty
        if train_length == 0:
            raise ValueError("Training data loader is empty!")

        for epoch in range(epochs):
            epoch_loss, epoch_score = 0, 0  # Fix the initialization here

            loop = tqdm(train_loader, position=0, colour="BLACK", leave=True, desc=f"Epoch [{epoch+1}/{epochs}]: ")
            for batch in loop:
                if self.device.type != "cpu":
                    torch.cuda.empty_cache()  # Clear GPU memory
                self.optimizer.zero_grad()
                self.model.train()

                images = batch["images"].float().to(self.device)
                labels = batch["labels"].long().to(self.device)

                probabilities = self.model(images).float().to(self.device)
                predictions = torch.argmax(probabilities, dim=1)

                batch_loss = self.criterion(probabilities, labels)
                epoch_loss += batch_loss.item()

                batch_score = self.metric(predictions, labels).item()
                epoch_score += batch_score

                batch_loss.backward()
                self.optimizer.step()

            # Check if train_length is zero before dividing
            if train_length == 0:
                print("Error: Division by zero during training. Train loader length is 0.")
                return

            epoch_loss /= train_length
            epoch_score /= train_length

            self.__log({"train_losses": epoch_loss, "train_scores": epoch_score})
            if self.logger is not None:
                self.logger.info(f"Epoch [{epoch+1}/{epochs}]: Loss: {epoch_loss} | Metric: {epoch_score}")

            if validation_loader is not None:
                validation_loss, validation_score = self.evaluate(validation_loader)
                self.__log({"validation_losses": validation_loss, "validation_scores": validation_score})
                if self.logger is not None:
                    self.logger.info(f"Validation Epoch [{epoch+1}/{epochs}]: Loss: {validation_loss} | Metric: {validation_score}")

                if self.scheduler is not None:
                    if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                        self.scheduler.step(validation_loss)
                    else:
                        self.scheduler.step()

                    if self.logger is not None:
                        lr = self.optimizer.param_groups[0]["lr"]
                        self.logger.info(f"Epoch [{epoch+1}/{epochs}] Learning Rate: {lr}")


trainer = Trainer(model=model,
                  criterion=criterion,
                  metric=accuracy_score,
                  optimizer=optimizer,
                  scheduler=scheduler,
                  logger=trainer_logger,
                  device=train_config.device)

trainer.fit(train_loader=train_loader, validation_loader=validation_loader, epochs=10)

model = PlantDiseaseModel(classes=len(train_dataset.labels))
criterion = nn.CrossEntropyLoss()
optimizer = get_optimizer(model=model,
                          name=train_config.optimizer.type,
                          parameters=train_config.optimizer.parameters)

if "scheduler" in train_config:
    scheduler = get_scheduler(optimizer=optimizer,
                              name=train_config.scheduler.type,
                              parameters=train_config.scheduler.parameters)

trainer_logger = get_logger("trainer")
trainer = Trainer(model=model,
                  criterion=criterion,
                  metric=accuracy_score,
                  optimizer=optimizer,
                  scheduler=scheduler,
                  logger=trainer_logger,
                  device=train_config.device)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Check the lengths and types first
print("Length of epochs_:", len(epochs_))
print("Length of train_losses:", len(trainer.history['train_losses']))
print("Length of validation_losses:", len(trainer.history['validation_losses']))

print("Type of epochs_:", type(epochs_))
print("Type of train_losses:", type(trainer.history['train_losses']))
print("Type of validation_losses:", type(trainer.history['validation_losses']))

# Convert losses to lists if they are not already
train_losses = list(trainer.history['train_losses'])
validation_losses = list(trainer.history['validation_losses'])

# Ensure validation_losses has the same length as train_losses
if len(validation_losses) < len(train_losses):
    validation_losses += [float('nan')] * (len(train_losses) - len(validation_losses))

# Check that lengths are now the same
print("Length of epochs_:", len(epochs_))
print("Length of train_losses:", len(train_losses))
print("Length of validation_losses:", len(validation_losses))

# Ensure there are no NaN or None values
print("NaN values in train_losses:", any(pd.isna(train_losses)))
print("NaN values in validation_losses:", any(pd.isna(validation_losses)))

# Plotting directly without using 'figure()'
#plt.grid(color="lightgrey", axis="both", alpha=0.8)

# Plot the train and validation losses
#sns.lineplot(x=epochs_, y=train_losses, color="red", marker="o", label="Train Loss", zorder=2)
#sns.lineplot(x=epochs_, y=validation_losses, color="blue", marker="o", label="Validation Loss", zorder=2)

# Set plot title and labels
#plt.title("Train & Validation Losses", fontsize=20, fontweight="bold", fontfamily="serif", loc="left", y=1.05)
#plt.xlabel("Epochs", fontsize=15, fontfamily="serif", labelpad=10)
#plt.ylabel("Loss", fontsize=15, fontfamily="serif", labelpad=5)

# Show legend
#plt.legend()

# Display the plot
#plt.show()

import pandas as pd
import json

# Assuming you have 'trainer.history.train_scores' and 'trainer.history.validation_scores' already populated

# Check the lengths of the data
print("Length of epochs:", len(epochs_))
print("Length of train_scores:", len(trainer.history['train_scores']))
print("Length of validation_scores:", len(trainer.history['validation_scores']))

# Ensure both scores are the same length
train_scores = list(trainer.history['train_scores'])
validation_scores = list(trainer.history['validation_scores'])

# If validation_scores is shorter than train_scores, pad it with NaN or 0
if len(validation_scores) < len(train_scores):
    validation_scores += [float('nan')] * (len(train_scores) - len(validation_scores))

# Print the accuracies for each epoch
print("\nTrain and Validation Accuracy per Epoch:")
for epoch, train_acc, validation_acc in zip(epochs_, train_scores, validation_scores):
    print(f"Epoch {epoch}: Train Accuracy = {train_acc}, Validation Accuracy = {validation_acc}")

# Option 1: Save the results to a CSV file
data = {
    'Epoch': epochs_,
    'Train Accuracy': train_scores,
    'Validation Accuracy': validation_scores
}

#df = pd.DataFrame(data)
#df.to_csv('accuracy_results.csv', index=False)
#print("\nAccuracy results saved to 'accuracy_results.csv'")

# Option 2: Save the results to a JSON file
accuracy_dict = {
    'epochs': epochs_,
    'train_scores': train_scores,
    'validation_scores': validation_scores
}

#with open('accuracy_results.json', 'w') as json_file:
    #json.dump(accuracy_dict, json_file)
#print("\nAccuracy results saved to 'accuracy_results.json'")

pip install streamlit torch torchvision Pillow

torch.save(model.state_dict(), 'PlantDiseaseModel.pth')

import streamlit as st
import torch
from torchvision import models, transforms
from PIL import Image

# Define the model architecture (same as used during training)
class PlantDiseaseModel(torch.nn.Module):
    def __init__(self):
        super(PlantDiseaseModel, self).__init__()
        self.resnet = models.resnet18(pretrained=True)  # Example: using ResNet18 architecture
        self.resnet.fc = torch.nn.Linear(self.resnet.fc.in_features, 3)  # Adjust to the number of classes you have

    def forward(self, x):
        return self.resnet(x)

# Initialize the model
model = PlantDiseaseModel()

# Load the trained model's weights
model.load_state_dict(torch.load('PlantDiseaseModel.pth', map_location=torch.device('cpu')))
model.eval()  # Set the model to evaluation mode

# Define the transformations for input images
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize image to match ResNet input size
    transforms.ToTensor(),  # Convert image to tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize like ResNet
])

# Streamlit App Interface
st.title("Plant Disease Classification")

st.write("This app classifies images of plant leaves into categories: Healthy, Rust, or Powdery Mildew.")

# Image upload functionality
uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    # Open and display the image
    image = Image.open(uploaded_file)
    st.image(image, caption="Uploaded Image", use_column_width=True)

    # Preprocess the image
    image_tensor = transform(image).unsqueeze(0)  # Add a batch dimension (required for model input)

    # Predict the class using the model
    with torch.no_grad():
        outputs = model(image_tensor)
        _, predicted_class = torch.max(outputs, 1)  # Get the class with the highest score

    # Define class names (adjust if needed based on your dataset)
    class_names = ['Healthy', 'Rust', 'Powdery']  # Example classes (make sure this matches your training dataset)

    # Display the predicted class
    st.write(f"Predicted class: {class_names[predicted_class.item()]}")

    # Optionally, show the class probabilities
    probabilities = torch.nn.functional.softmax(outputs, dim=1).squeeze().numpy()
    st.write(f"Class probabilities: Healthy: {probabilities[0]:.4f}, Rust: {probabilities[1]:.4f}, Powdery: {probabilities[2]:.4f}")

!pip install streamlit
!pip install torch torchvision pillow

!streamlit run /content/pdc.ipynb